---
title: "R Notebook"
output: html_notebook
---

# Unit 1: Intro to Analytics 

## Getting Started in R

```{r}
# Get help on functions
?sqrt

# Get a list of vars in env
ls()
```


## Vectors and DataFrames

```{r}
# Create a vector using c function
country <- c("Brazil", "China", "India", "Switzerland", "USA")
LifeExpectancy <- c(74, 76, 65, 83, 79)

# To display an element of a vec, use [ ]
country[3]

# To create a sequence, use seq
seq(0,100,2)

# Combine vecs into a df structure
CountryData <- data.frame(country, LifeExpectancy)
CountryData

# Now, add a var to this df. Use a $ to link the data
CountryData$Population <- c(199000,1390000, 1240000,7997,318000)

CountryData

# Now, add 2 new countries: Aus and Grc. 
country <- c("Australia", "Greece")
LifeExpectancy <- c(82,81)
Population <- c(23050,11125)

NewCountryData <- data.frame(country, LifeExpectancy, Population)
NewCountryData

# Now, combine CountryData and NewCountryData
AllCountryData <- rbind(CountryData, NewCountryData)
AllCountryData
```

## Loading Data Files

```{r}
getwd()
WHO <- read.csv("WHO.csv")
str(WHO)

summary(WHO)

# Subsetting data to include only countries in EU
?subset
WHO_Europe <- subset(WHO,Region = "Europe")
WHO_Europe

# Now, remove WHO_Europe from environment
rm(WHO_Europe)
ls()

```

## Data Analysis - Summary Stats and Scatterplots

```{r}
mean(WHO$Under15)
sd(WHO$Under15)
summary(WHO$Under15)  #13.12% of pop is under 15 (min) - which one is it?  Japan!
WHO$Country[which.min(WHO$Under15)]

# Which country has max under 15? Niger with 49.99!
WHO$Country[which.max(WHO$Under15)]

# Create a scatterplot of GNI vs. fertility rate
plot(WHO$GNI, WHO$FertilityRate)

# look at countries with a GNI > 10,000 and Fertility Rate > 2.5
outliers <- subset(WHO, GNI > 10000 & FertilityRate > 2.5)
nrow(outliers)

outliers[c("Country", "GNI", "FertilityRate")]

# What is the mean value of WHO$Over60? 11.16366
mean(WHO$Over60)

# Which country has the smallest % of pop over 60?
WHO$Country[which.min(WHO$Over60)]

# which country has the largest literacy rate?
WHO$Country[which.max(WHO$LiteracyRate)]

```

## Data Analysis - Plots and Summary Tables

IQR = Difference between 1st and 3rd quartiles  
Any point that is > 3rd quartile + IQR or < 1st quartile - IQR is an outlier.

1. EU has highest median life expectancy.  
2. Americas has the smallest IQR.  
3. Eastern Mediterranean has the highest overall range of life expectancy.  



```{r}
# Histograms

hist(WHO$CellularSubscribers)

# Boxplot

boxplot(WHO$LifeExpectancy~WHO$Region)

#  Add labels
boxplot(WHO$LifeExpectancy~WHO$Region, xlab = "", ylab = "Life Expectancy", main = "Life Expectancy of Countries by Region")

# Summary Tables

table(WHO$Region)

# tapply...splits data by 2nd arg, then applies the 3rd to the var given as 1st. Take the mean of those > 60 by region.

tapply(WHO$Over60, WHO$Region, mean)

tapply(WHO$LiteracyRate, WHO$Region, min, na.rm=T)

# Use tapply to find the average child mortality rate of countries in each region.
tapply(WHO$ChildMortality, WHO$Region, mean)

```

## Recitation 1 - Nutrition
In 1990, all states had < 14% obesity. By 2000, half of the country had > 20% of its population obese.  By 2010, all states had > 20% of its population obese.

```{r}
USDA <- read.csv("USDA.csv")
str(USDA)
summary(USDA)

# The max of sodium is very high, which value does it correspond to?
USDA$Description[which.max(USDA$Sodium)]

# Which foods contain > 10,000 mg of sodium?
HighSodium <- subset(USDA, Sodium > 10000)
nrow(HighSodium)
HighSodium$Description

# How much salt does caviar have? 1500
USDA$Sodium[match("CAVIAR", USDA$Description)]

# Compare Caviar to mean and sd across dataset. Mean sodium = 322.1, SD Sodium= 1045.417
# 322 + 1045 = 1367 -> caviar is > 1 SD above the mean
summary(USDA$Sodium)
sd(USDA$Sodium, na.rm = T)

## Creating Plots in R 

# Create a scatterplot w/ Protien on the x-axis and Fat on the y-axis
plot(USDA$Protein, USDA$TotalFat, xlab="Protien", ylab= "Total Fat", main = "Protien vs Fat", col = "blue")

# Create a hist of Vitamin C
hist(USDA$VitaminC, main = "Histogram of Vitamin C Levels")

# Need to zoom in on < 100mg

hist(USDA$VitaminC, main = "Histogram of Vitamin C Levels", xlim = c(0,100))

# Now, break up into 100 increments.  We only see 5 cells and each cell is 20mg?  Original went to 2000mg.  2000mg/100 = 20mg
?hist
hist(USDA$VitaminC, main = "Histogram of Vitamin C Levels", xlim = c(0,100), breaks = 100)

# let's get to 100 breaks
hist(USDA$VitaminC, main = "Histogram of Vitamin C Levels", xlim = c(0,100), breaks = 2000, col = "pink")

# boxplot for sugar
boxplot(USDA$Sugar, main = "Boxplot of Sugar Levels", ylab = "Sugar in grams")

# Adding variables 
# Add a 1 if food is higher than sodium avg and 0 if less than sodium average
USDA$Sodium[1] > mean(USDA$Sodium, na.rm = T)
USDA$Sodium[50] > mean(USDA$Sodium, na.rm = T)
USDA$HighSodium <- as.numeric(USDA$Sodium > mean(USDA$Sodium, na.rm = TRUE))
str(HighSodium)
USDA$HighProtien <- as.numeric(USDA$Protein > mean(USDA$Protein, na.rm = TRUE))
USDA$HighFat <- as.numeric(USDA$TotalFat > mean(USDA$TotalFat, na.rm = TRUE))
USDA$HighCarbs <- as.numeric(USDA$Carbohydrate > mean(USDA$Carbohydrate, na.rm = TRUE))
str(USDA)

# Summary Tables
## Now, figure out how many foods have a higher sodium level than average
table(USDA$HighSodium)

## Now, how many have High Sodium and High Fat? Rows are first input, cols are second
table(USDA$HighSodium, USDA$HighFat)

## Now, what if wanted avg amount of Iron sorted by Hi and Lo protien? NEED TAPPLY? WHY?
## tapply Groups arg 1 by arg 2 and applies arg 3

tapply(USDA$Iron, USDA$HighProtien, mean, na.rm = TRUE)

## What is the max level of Vitamin C in foods with Hi and Lo carbs?
tapply(USDA$VitaminC, USDA$HighCarbs, max, na.rm = TRUE)

## is it true that foods high in carbs have a high vitamin C content? Yes, on average
tapply(USDA$VitaminC, USDA$HighCarbs, summary, na.rm = TRUE)

```


## HW - An Analytical Detective

```{r}
mvt <- read.csv("mvtWeek1.csv")

#How many rows?
str(mvt)

#Max of ID?
max(mvt$ID)

# Min of beat?
min(mvt$Beat)

# How many T arrest var?
summary(mvt$Arrest)

# How many have location == alley?
sum(mvt$LocationDescription == "ALLEY")
```

### Dates in R


```{r}
# Look at the first date in Date....Month/Day/Year 24Hour:Minute
mvt$Date[1]

# Convert chars into a date object in R for Date col into DateConvert col
DateConvert <- as.Date(strptime(mvt$Date, "%m/%d/%y %H:%M"))
summary(DateConvert)

# Now, let's create two new variables in our dataframe, Month and Weekday
mvt$Month <- months(DateConvert)
mvt$Weekday <- weekdays(DateConvert)
mvt$Date <- DateConvert

# In which month did the fewest motor vehicle thefts occur?
table(mvt$Month)

# Which weekday had the most?
table(mvt$Weekday)

# Which month has the largest number of mvt for which an arrest was made?
summary(mvt$Arrest)
table(mvt$Arrest, mvt$Month)
```


### Visualizing Crime Trends

```{r}
# Does crime increase or decrease from 2002 - 2012?
hist(mvt$Date, breaks=100)

# Create a boxplot of Date sorted by Arrest (YIKES!)
?boxplot
boxplot(mvt$Date ~ mvt$Arrest)

# If you look at the boxplot, the one for Arrest=TRUE is definitely skewed towards the bottom of the plot

# For what proportion of motor vehicle thefts in 2001 was an arrest made? Value btw 0 and 1
# Arrest = T / all incidents in 2001
table(mvt$Arrest, mvt$Year)
2152/(2152+18517) #2001
1212/(13068+1212) #2007
550/(13542+550) #2012

```


```{r}
# Popular locations

## If Chi wants to increase # of arrests, where should they focus?
## Create a table of LocationDescription using sort
sort(table(mvt$LocationDescription), decreasing = TRUE)

## Popular locations
Top5 <- subset(mvt, LocationDescription == "STREET" | LocationDescription == "PARKING LOT/GARAGE(NON.RESID.)" | LocationDescription == "ALLEY" | LocationDescription == "GAS STATION" | LocationDescription == "DRIVEWAY - RESIDENTIAL")
nrow(Top5)

## Refresh

Top5$LocationDescription <- factor(Top5$LocationDescription)
str(Top5)

# Which location has higher arrest rate than others?

table(Top5$Arrest, Top5$LocationDescription)

# Which day of the week do most mvt at gas stations happen?
table(Top5$Weekday,Top5$LocationDescription == "GAS STATION")

# Which day of the week do FEWEST mvt at DRIVEWAYs happen?
table(Top5$Weekday,Top5$LocationDescription == "DRIVEWAY - RESIDENTIAL")
```


```{r}
# Stock Dynamics
IBM <- read.csv("IBMStock.csv")
GE <- read.csv("GEStock.csv")
ProcterGamble <- read.csv("ProcterGambleStock.csv")
CocaCola <- read.csv("CocaColaStock.csv")
Boeing <- read.csv("BoeingStock.csv")

# Convert Date field into something R understands
IBM$Date = as.Date(IBM$Date, "%m/%d/%y")
GE$Date = as.Date(GE$Date, "%m/%d/%y")
CocaCola$Date = as.Date(CocaCola$Date, "%m/%d/%y")
ProcterGamble$Date = as.Date(ProcterGamble$Date, "%m/%d/%y")
Boeing$Date = as.Date(Boeing$Date, "%m/%d/%y")

# How many observations are there?
nrow(IBM)

# What is the earliest year?
min(IBM$Date)
min(Boeing$Date)

# Latest year?
max(IBM$Date)

# Mean IBM price?
mean(IBM$StockPrice)

# Min GE price?
min(GE$StockPrice)

# Max Coke price?
max(CocaCola$StockPrice)

# Median Boeing?
summary(Boeing)

# SD of P&G?
sd(ProcterGamble$StockPrice)

```


### Visualizing Stock Dynamics

```{r}
# Plot Coke - Date on x-axis, price on y-axis using a line
plot(CocaCola$Date, CocaCola$StockPrice, type = "l", col = "red")

# Now, add P&G to Coke
lines(ProcterGamble$Date, ProcterGamble$StockPrice, col = "blue")
abline(v=as.Date(c("1983-03-01")), lwd=1)


# Changes from 95 to 2005. Use lines function to add other companies
plot(CocaCola$Date[301:432], CocaCola$StockPrice[301:432], type="l", col="red", ylim=c(0,210))
lines(ProcterGamble$Date, ProcterGamble$StockPrice, col = "blue")
lines(IBM$Date, IBM$StockPrice, col = "black")
lines(Boeing$Date, Boeing$StockPrice, col = "green")
lines(GE$Date, GE$StockPrice, col = "orange")
abline(v=as.Date(c("2004-01-01")), lwd=1)
abline(v=as.Date(c("2005-11-30")), lwd=1)


## Monthly trends
## Calculate the mean price of IBM sorted by month using tapply

## tapply Groups arg 1 by arg 2 and applies arg 3

tapply(IBM$StockPrice, months(IBM$Date), mean, na.rm = TRUE)
mean(IBM$StockPrice)


## Repeat above for Coke
tapply(CocaCola$StockPrice, months(CocaCola$Date), mean, na.rm = TRUE)
mean(CocaCola$StockPrice)

tapply(GE$StockPrice, months(GE$Date), mean, na.rm = TRUE)
60.02973
```

```{r}
## Demographics and Employment in the United States
## How many interviewees are in the dataset?

CPS <- read.csv("CPSData.csv")
nrow(CPS)

## What is most common industry of employment?  Try sorting!
  
str(CPS)
sort(table(CPS$Industry),decreasing = TRUE)


## Which state has the fewest interviewees?

sort(table(CPS$State), decreasing = TRUE)

## What PROPORTION of interviewees are US citizens?

table(CPS$Citizenship)
(116639+7073)/nrow(CPS)

## For which races are there at least 250 interviewees in the CPS dataset of Hispanic ethnicity?

table(CPS$Hispanic, CPS$Race)

## Which variables have at least one interviewee with a missing (NA) value? (Select all that apply.)
summary(CPS)

## Is there a pattern in missing values?  Is Married based upon region?
table(CPS$Region, is.na(CPS$Married))
table(CPS$Sex, is.na(CPS$Married))
table(CPS$Age, is.na(CPS$Married))

### Region
6075/(24609+6075)
4507/(21432+4507)
7967/(33535+7967)
6789/(26388+6789)

### Sex
12217/(12217+55264)
13121/(13121+50700)

## MetroCode is N/A if interviewee does not live in a metro area. 
## How many states had all interviewees living in a non-metro area (missing metro area)
table(is.na(CPS$MetroAreaCode), CPS$State)

## Which region has largest proportion living in a non-metro area?
table(is.na(CPS$MetroAreaCode), CPS$Region)

## Less tedious way to calculate proportions that are TRUE!  Mean returns proportion of values that are TRUE!
sort(tapply(is.na(CPS$MetroAreaCode), CPS$State, mean), decreasing = TRUE)

## Read in 2 dictionaries
MetroAreaMap <- read.csv("MetroAreaCodes.csv")
str(MetroAreaMap)

CountryMap <- read.csv("CountryCodes.csv")
str(CountryMap)


## Merge and overwrite the CPS df

CPS = merge(CPS, MetroAreaMap, by.x="MetroAreaCode", by.y="Code", all.x=TRUE)

## What is the name of the new var that was added to CPS?
summary(CPS)
str(CPS)

## How many interviewees have a missing value for the new MetroArea variable?
table(is.na(CPS$MetroArea))

## Which metro area has the largest # of interviewees?
sort(table(CPS$MetroArea), decreasing = TRUE)

## Which  metro area has the > proportion of Hispanics?
sort(tapply(CPS$Hispanic, CPS$MetroArea, mean, na.rm = TRUE), decreasing = TRUE)

##  determine the number of metropolitan areas in the United States from which at least 20% of interviewees are Asian.
sort(tapply(CPS$Race == "Asian", CPS$MetroArea, mean, na.rm = TRUE), decreasing = TRUE)

##  determine which metropolitan area has the smallest proportion of interviewees who have received no high school diploma

sort(tapply(CPS$Education == "No high school diploma", CPS$MetroArea, mean, na.rm = TRUE), decreasing = FALSE)


## merge in the country of birth information from the CountryMap data frame
CPS <- merge(CPS, CountryMap, by.x="CountryOfBirthCode", by.y="Code", all.x=TRUE)

## How many interviewees have a missing value for the new country of birth variable?
table(is.na(CPS$Country))

## Which country outside of NA was most common?
sort(table(CPS$Country), decreasing = TRUE)

## What proportion of the interviewees from the "New York-Northern New Jersey-Long Island, NY-NJ-PA" metropolitan area have a country of birth that is not the United States?

#table(subset(CPS, Country == "United States"), subset(CPS, MetroArea == "New York-Northern New Jersey-Long Island, NY-NJ-PA"))

####  table(CPS$MetroArea == "New York-Northern New Jersey-Long Island, NY-NJ-PA", CPS$Country != "United States")

MetroNJ <- subset(CPS, MetroArea == "New York-Northern New Jersey-Long Island, NY-NJ-PA")
head(MetroNJ)

table(MetroNJ$Country == "United States")

1668/nrow(MetroNJ)

## Which metropolitan area has the largest number (note -- not proportion) of interviewees with a country of birth in India? 

###To obtain the number of TRUE values in a vector of TRUE/FALSE values, you can use the sum() function. For instance, sum(c(TRUE, FALSE, TRUE, TRUE)) is 3. Therefore, we can obtain counts of people born in a particular country living in a particular metropolitan area with:



```
## Summary of Lessons from Unit 1

**Analytics** is the science of using **data** to build **models** that lead to better **decisions** that add **value** to individuals, companies, and institutions.  

**IQR** = Difference between 1st and 3rd quartiles  
**Outlier** = Any point that is > 3rd quartile + IQR or < 1st quartile - IQR is an outlier. 

**tapply** = splits data by 2nd arg, then applies the 3rd function to the var given as 1st.  ```{r}tapply(WHO$LiteracyRate, WHO$Region, min, na.rm=T)```




**Basic** functions:  
1. ?function = help page for function  
2. ls() = list objects in enviro; rm(var) = remove object from enviro    
3. data.frame(vec1, vec2) = creates a df from vecs  
4. subset(data, col1 > 100 | col2 < 500) - can use & or |  
5. ```{r} WHO$Country[which.min(WHO$Over60)]``` - look up Country with min Over60  
6. 


```{r}

```

## Linear Regression in R  

```{r}
wine <- read.csv("wine.csv")
str(wine)
summary(wine)

# Create a one-var linear regression using AGST to predict price
## lm function = linear model  lm(dependent var ~ independent,)

model1 <- lm(Price ~ AGST, data = wine)
summary(model1)

## Let's see the value of our residuals
model1$residuals

SSE <- sum(model1$residuals^2)
SSE

## Now, let's add harvest rain to our regression model
model2 <- lm(Price ~ AGST + HarvestRain, data = wine)
summary(model2)

SSE <- sum(model2$residuals^2)
SSE

## Now, use ALL independent variables
model3 <- lm(Price ~ AGST + WinterRain + HarvestRain + Age + FrancePop, data = wine)
summary(model3)
SSE <- sum(model3$residuals^2)
SSE

## Predict Price using HarvestRain & WinterRain
modelHW <- lm(Price ~ HarvestRain + WinterRain, data = wine)
summary(modelHW)

# Understanding the model  
## Let's remove francepop because jit is insignificant
model4 <- lm(Price ~ AGST + WinterRain + HarvestRain + Age, data = wine)
summary(model4)

# HW question 
modelHW2 <- lm(Price ~ HarvestRain + WinterRain, data = wine)
summary(modelHW2)

# HW question
cor(wine$HarvestRain, wine$WinterRain)

# Now, see how good model is w/ test data  

wineTest <- read.csv("wine_test.csv")
str(wineTest)

# To make predictions, use predict
predictTest <- predict(model4, newdata = wineTest)
predictTest

# We can compare test w/ actual data in test set 1. 6.769 vs 6.95 2. 6.685 vs 6.5
## Now, quantify using R^2

SSE <- sum((wineTest$Price - predictTest)^2)
SST <- sum((wineTest$Price - mean(wine$Price))^2)
1 - SSE/SST
```


## Baseball section  

```{r}
baseball <- read.csv("baseball.csv")
str(baseball)

## subset data to only include years < 2002

moneyball <- subset(baseball, Year < 2002)
summary(moneyball)

## Goal = predict wins using the difference btw runs scored and runs allowed
## create a new variable for above
moneyball$RD <- moneyball$RS - moneyball$RA
str(moneyball)

## before we run LR, let's check to see if there is a LR btw RD and Wins
plot(moneyball$RD, moneyball$W)

WinsReg <- lm(W ~ RD, data = moneyball)
summary(WinsReg)

## Now, use model to confirm a team needs at least 135 runs than they allow to win 95 games

str(moneyball)

## can we predict RS using OBP, SLG, and BA?
RunsReg <- lm(RS ~ OBP + SLG + BA, data = moneyball)
summary(RunsReg)

RunsReg <- lm(RS ~ OBP + SLG, data = moneyball)
summary(RunsReg)

## if OBP = 0.311 and SLG = 0.405, how many runs?
-804.63 + (2737.77 * 0.311) + (1584.91 * 0.405)

## Allowing runs
RunsAllow <- lm(RA ~ OOBP + OSLG, data = moneyball)
summary(RunsAllow)

## If OOBP = 0.297 and OSLG = 0.370, how many runs do we expect?
-837.38 + (2913.60 * 0.297) + (1514.29 * 0.370)

## which two player for $1.5M?  Max runs at $1.5M
## Chavez OBP = 0.338; SLG = 0.540; $1.4M -> 977
## Giambi OBP = 0.391; SLG = 0.450; $1.065M -> 979.0476
## Frank  OBP = 0.369; SLG = 0.374; $0.295M -> 793
## Greg   OBP = 0.313; SLG = 0.447; $0.800M -> 761
## Carlos OBP = 0.361; SLG = 0.5; $0.300M -> 976.16

-804.63 + (2737.77 * 0.361) + (1584.91 * 0.5)

979+793

## Ranking question

teamRank = c(1,2,3,3,4,4,4,4,5,5)

wins2012 <- c(94, 88, 95, 88, 93, 94, 98, 97, 93, 94)

wins2013 <- c(97, 97, 92, 93, 92, 96, 94, 96, 92, 90)

cor(teamRank,wins2012)
cor(teamRank,wins2013)

```


## Predicting Runs and Wins  
```{r}
## Can we predict how many games 2002 A's will win using our models?
## If b4 season starts, use prediction on 2001 perf
## Using 2001 stats, Team OBP = 0.339 and Team SLG = 0.430
RS <- -804.63 + (2737.77 * 0.339) + (1584.91 * 0.430)
## RS = 805 runs

## Now, RA  OOBP = 0.307 and OSLG = 0.373 --> RA = 622
RA <- -837.38 + (2913.60 * 0.307) + (1514.29 * 0.373)
RA

## 183 
RD <- 805 - 622
RD

## Wins prediction -> 100 W
W <- 80.8814 + 0.1058 * (183)
W


```


## Week 2 Homework

```{r}
## Problem 1.1 
climateChange <- read.csv("climate_change.csv")

### Create a training set and testing set
trainCC <- subset(climateChange, Year <= 2006)
testCC <- subset(climateChange, Year > 2006)


CCModel1 <- lm(Temp ~ MEI + CO2 + CH4 + N2O + CFC.11 + CFC.12 + TSI + Aerosols, data = trainCC )

summary(CCModel1)

## Problem 2.2 - compute cor of training set
cor(trainCC)

## Problem 3 - simplify model

CCModel2 <- lm(Temp ~ MEI + N2O + TSI + Aerosols, data = trainCC )
summary(CCModel2)


## Problem 4 - Automatically build the model using 'step' fn.  Tradeoff using AIC model.
## use step fn to derive a new model
stepModel <- step(CCModel1)
summary(stepModel)

## Problem 5 - Testing on Unseen Data - calculate temperature predictions for the testing data set, using the predict function.
## ref predictTest <- predict(model4, newdata = wineTest)
### SSE <- sum((wineTest$Price - predictTest)^2)
### SST <- sum((wineTest$Price - mean(wine$Price))^2)
### 1 - SSE/SST

predictCC <- predict(stepModel, newdata = testCC)
summary(predictCC)
str(predictCC)

SSE_CC <- sum((testCC$Temp - predictCC)^2)
SST_CC <- sum((testCC$Temp - mean(trainCC$Temp))^2)
1 - SSE_CC/SST_CC

```



## Problem 2 - Reading Test Scores

```{r}
## Problem 1
pisaTrain <- read.csv("pisa2009train.csv")
pisaTest <- read.csv("pisa2009test.csv")
str(pisaTrain)

## Prob 2 - using tapply, what is avg reading test score of males? Of females?
### # tapply...splits data by 2nd arg, then applies the 3rd to the var given as 1st. Take the mean of those > 60 by region.

### tapply(WHO$Over60, WHO$Region, mean)
tapply(pisaTrain$readingScore, pisaTrain$male == 1, mean)

## Prob 3 - missing values
summary(pisaTrain)

## Prob 4 - remove missing values
## remove all N/A's from train and test data
pisaTrain = na.omit(pisaTrain)
pisaTest = na.omit(pisaTest)

str(pisaTrain)
str(pisaTest)

## Prob 2.1 Factor variables - which is unordered factor with at least 3 levels?

## Prob 2.2 - Unordered factors in regression models

## Prob 3.1 - Building a model
### Set ref level to white in train / test

pisaTrain$raceeth <- relevel(pisaTrain$raceeth, "White")
pisaTest$raceeth <- relevel(pisaTest$raceeth, "White")

## build model w/ train to predict reading score using all vars

 lmScore <- lm(readingScore ~ ., data = pisaTrain)
 summary(lmScore)
 
## What is RMSE of lmScore?
## RMSE =  > RMSE = sqrt(SSE/nrow(NBA))
### Use to compute SSE:  > SSE = sum(PointsReg$residuals^2)

lmScore$residuals 
SSE_pisa <- sum(lmScore$residuals^2)
summary(lmScore)
RMSE_pisa <- sqrt(SSE_pisa/nrow(pisaTrain))
RMSE_pisa

## Problem 3.3 - comparing predictions for similar students
## reading score prediction - only grade is different
# predict A
A <- 143.7663 + 29.5427 * (11)
B <- 143.7663 + 29.5427 * (9)
A - B


## Prob 4.1 - Predicting on unseen data
## use lmscore to predict reading scores of students in pisaTest
## predictTest <- predict(model4, newdata = wineTest)


predTest <- predict(lmScore, newdata = pisaTest)
summary(predTest)
str(predTest)
max(predTest) - min(predTest)

## Prob 4.2 - Test Set SSE and RMSE
## SSE of lmscore on testing set?
## RMSE of lmscore on testing set?

## lmScore$residuals 
## SSE_pisa <- sum(lmScore$residuals^2)
## summary(lmScore)
## RMSE_pisa <- sqrt(SSE_pisa/nrow(pisaTrain))
## RMSE_pisa

## The training-set RMSE can be computed by first computing the SSE:
## SSE = sum(lmScore$residuals^2) 
## and then dividing by the number of observations and taking the square root:
# RMSE = sqrt(SSE / nrow(pisaTrain))
 

SSE_pisa_test <- sum((predTest - pisaTest$readingScore)^2)
SSE_pisa_test
RMSE_pisa_test <- sqrt(SSE_pisa_test/nrow(pisaTest))
RMSE_pisa_test


## Prob 4.3 - Baseline prediction and test set SSE
## what is predicted test score using the training set?  WHY???

baseline <- mean(pisaTrain$readingScore)
baseline


 sum((baseline-pisaTest$readingScore)^2)
 
 ## Prob 4.4 - what is test-set R^2 value of lmscore?
SST_pisa_test <- sum(pisaTest$readingScore - mean(pisaTrain$readingScore)^2)
SST_pisa_test
 1 - (SSE_pisa_test / SST_pisa_test)
```


## Google Flu Trends 

```{r}
## 1.1 Understanding the data

FluTrain <- read.csv("FluTrain.csv")
str(FluTrain)

## which week is highest %age of ILI-related physician visits?
### WHO$Country[which.min(WHO$Under15)]

FluTrain$Week[which.max(FluTrain$ILI)]

FluTrain$Week[which.max(FluTrain$Queries)]

## Plot histogram of dependent var ILI

hist(FluTrain$ILI)

## Understanding the data - skewed -> predict log of dep. var
## Plot the natural log of ILI vs Queries

plot(log(FluTrain$ILI), FluTrain$Queries)

## Run lm - what is R^2?
FluTrend1 <- lm(log(ILI) ~ Queries, data = FluTrain)
summary(FluTrend1)

## cor


## Perf on Test Set
FluTest <- read.csv("FluTest.csv")
PredTest1 = exp(predict(FluTrend1, newdata=FluTest))

##
est_ILI <- PredTest1[which(FluTest$Week == "2012-03-11 - 2012-03-17")]


## 3.2 - Relative Error?  (Observed ILI - Estimated ILI)/Observed ILI
(FluTest$ILI[11]-est_ILI)/FluTest$ILI[11]

install.packages("zoo")
library(zoo)

ILILag2 = lag(zoo(FluTrain$ILI), -2, na.pad=TRUE)
FluTrain$ILILag2 = coredata(ILILag2)

## 4.2
plot(log(FluTrain$ILILag2), log(FluTrain$ILI))

```

This is an [R Markdown](http://rmarkdown.rstudio.com) Notebook. When you execute code within the notebook, the results appear beneath the code. 

Try executing this chunk by clicking the *Run* button within the chunk or by placing your cursor inside it and pressing *Ctrl+Shift+Enter*. 

```{r}
plot(cars)
```

Add a new chunk by clicking the *Insert Chunk* button on the toolbar or by pressing *Ctrl+Alt+I*.

When you save the notebook, an HTML file containing the code and output will be saved alongside it (click the *Preview* button or press *Ctrl+Shift+K* to preview the HTML file).

The preview shows you a rendered HTML copy of the contents of the editor. Consequently, unlike *Knit*, *Preview* does not run any R code chunks. Instead, the output of the chunk when it was last run in the editor is displayed.